{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "# R precision\n",
    "def r_precision(targets, predictions, max_n_predictions=500):\n",
    "    # Assumes predictions are sorted by relevance\n",
    "    # First, cap the number of predictions\n",
    "    predictions = predictions[:max_n_predictions]\n",
    "\n",
    "    # Calculate metric\n",
    "    target_set = set(targets)\n",
    "    target_count = len(target_set)\n",
    "    return float(len(set(predictions[:target_count]).intersection(target_set))) / target_count\n",
    "\n",
    "def dcg(relevant_elements, retrieved_elements, k, *args, **kwargs):\n",
    "    \"\"\"Compute the Discounted Cumulative Gain.\n",
    "    Rewards elements being retrieved in descending order of relevance.\n",
    "    \\[ DCG = rel_1 + \\sum_{i=2}^{|R|} \\frac{rel_i}{\\log_2(i + 1)} \\]\n",
    "    Args:\n",
    "        retrieved_elements (list): List of retrieved elements\n",
    "        relevant_elements (list): List of relevant elements\n",
    "        k (int): 1-based index of the maximum element in retrieved_elements\n",
    "        taken in the computation\n",
    "    Note: The vector `retrieved_elements` is truncated at first, THEN\n",
    "    deduplication is done, keeping only the first occurence of each element.\n",
    "    Returns:\n",
    "        DCG value\n",
    "    \"\"\"\n",
    "    retrieved_elements = __get_unique(retrieved_elements[:k])\n",
    "    relevant_elements = __get_unique(relevant_elements)\n",
    "    if len(retrieved_elements) == 0 or len(relevant_elements) == 0:\n",
    "        return 0.0\n",
    "    # Computes an ordered vector of 1.0 and 0.0\n",
    "    score = [float(el in relevant_elements) for el in retrieved_elements]\n",
    "    # return score[0] + np.sum(score[1:] / np.log2(\n",
    "    #     1 + np.arange(2, len(score) + 1)))\n",
    "    return np.sum(score / np.log2(1 + np.arange(1, len(score) + 1)))\n",
    "\n",
    "\n",
    "def ndcg(relevant_elements, retrieved_elements, k, *args, **kwargs):\n",
    "    \"\"\"Compute the Normalized Discounted Cumulative Gain.\n",
    "    Rewards elements being retrieved in descending order of relevance.\n",
    "    The metric is determined by calculating the DCG and dividing it by the\n",
    "    ideal or optimal DCG in the case that all recommended tracks are relevant.\n",
    "    Note:\n",
    "    The ideal DCG or IDCG is on our case equal to:\n",
    "    \\[ IDCG = 1+\\sum_{i=2}^{min(\\left| G \\right|, k)}\\frac{1}{\\log_2(i +1)}\\]\n",
    "    If the size of the set intersection of \\( G \\) and \\( R \\), is empty, then\n",
    "    the IDCG is equal to 0. The NDCG metric is now calculated as:\n",
    "    \\[ NDCG = \\frac{DCG}{IDCG + \\delta} \\]\n",
    "    with \\( \\delta \\) a (very) small constant.\n",
    "    The vector `retrieved_elements` is truncated at first, THEN\n",
    "    deduplication is done, keeping only the first occurence of each element.\n",
    "    Args:\n",
    "        retrieved_elements (list): List of retrieved elements\n",
    "        relevant_elements (list): List of relevant elements\n",
    "        k (int): 1-based index of the maximum element in retrieved_elements\n",
    "        taken in the computation\n",
    "    Returns:\n",
    "        NDCG value\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: When https://github.com/scikit-learn/scikit-learn/pull/9951 is\n",
    "    # merged...\n",
    "    idcg = dcg(\n",
    "        relevant_elements, relevant_elements, min(k, len(relevant_elements)))\n",
    "    if idcg == 0:\n",
    "        raise ValueError(\"relevent_elements is empty, the metric is\"\n",
    "                         \"not defined\")\n",
    "    true_dcg = dcg(relevant_elements, retrieved_elements, k)\n",
    "    return true_dcg / idcg\n",
    "\n",
    "\n",
    "def __get_unique(original_list):\n",
    "    \"\"\"Get only unique values of a list but keep the order of the first\n",
    "    occurence of each element\n",
    "    \"\"\"\n",
    "    return list(OrderedDict.fromkeys(original_list))\n",
    "\n",
    "\n",
    "Metrics = namedtuple('Metrics', ['r_precision', 'ndcg', 'plex_clicks'])\n",
    "\n",
    "\n",
    "# playlist extender clicks\n",
    "def playlist_extender_clicks(targets, predictions, max_n_predictions=500):\n",
    "    # Assumes predictions are sorted by relevance\n",
    "    # First, cap the number of predictions\n",
    "    predictions = predictions[:max_n_predictions]\n",
    "\n",
    "    # Calculate metric\n",
    "    i = set(predictions).intersection(set(targets))\n",
    "    for index, t in enumerate(predictions):\n",
    "        for track in i:\n",
    "            if t == track:\n",
    "                return float(int(index / 10))\n",
    "    return float(max_n_predictions / 10.0 + 1)\n",
    "\n",
    "\n",
    "# def compute all metrics\n",
    "def get_all_metrics(targets, predictions, k):\n",
    "    return Metrics(r_precision(targets, predictions, k),\n",
    "                   ndcg(targets, predictions, k),\n",
    "                   playlist_extender_clicks(targets, predictions, k))\n",
    "\n",
    "\n",
    "MetricsSummary = namedtuple('MetricsSummary', ['mean_r_precision',\n",
    "                                               'mean_ndcg',\n",
    "                                               'mean_plex_clicks',\n",
    "                                               'coverage'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# EXCEPTION in query 759505: IndriRunQuery.cpp(410): QueryThread::_runQuery Exception\n",
      "# EXCEPTION in query 864483: IndriRunQuery.cpp(410): QueryThread::_runQuery Exception\n",
      "# EXCEPTION in query 110994: IndriRunQuery.cpp(410): QueryThread::_runQuery Exception\n",
      "# EXCEPTION in query 591668: IndriRunQuery.cpp(410): QueryThread::_runQuery Exception\n",
      "# EXCEPTION in query 754459: IndriRunQuery.cpp(410): QueryThread::_runQuery Exception\n",
      "# EXCEPTION in query 479140: IndriRunQuery.cpp(410): QueryThread::_runQuery Exception\n",
      "# EXCEPTION in query 221635: IndriRunQuery.cpp(410): QueryThread::_runQuery Exception\n",
      "# EXCEPTION in query 841212: IndriRunQuery.cpp(410): QueryThread::_runQuery Exception\n",
      "# EXCEPTION in query 943553: IndriRunQuery.cpp(410): QueryThread::_runQuery Exception\n",
      "# EXCEPTION in query 512541: IndriRunQuery.cpp(410): QueryThread::_runQuery Exception\n",
      "# EXCEPTION in query 841351: IndriRunQuery.cpp(410): QueryThread::_runQuery Exception\n",
      "# EXCEPTION in query 609540: IndriRunQuery.cpp(410): QueryThread::_runQuery Exception\n",
      "# EXCEPTION in query 603765: IndriRunQuery.cpp(410): QueryThread::_runQuery Exception\n",
      "# EXCEPTION in query 593650: IndriRunQuery.cpp(410): QueryThread::_runQuery Exception\n",
      "# EXCEPTION in query 759505: IndriRunQuery.cpp(410): QueryThread::_runQuery Exception\n",
      "# EXCEPTION in query 864483: IndriRunQuery.cpp(410): QueryThread::_runQuery Exception\n",
      "# EXCEPTION in query 110994: IndriRunQuery.cpp(410): QueryThread::_runQuery Exception\n",
      "# EXCEPTION in query 591668: IndriRunQuery.cpp(410): QueryThread::_runQuery Exception\n",
      "# EXCEPTION in query 754459: IndriRunQuery.cpp(410): QueryThread::_runQuery Exception\n",
      "# EXCEPTION in query 479140: IndriRunQuery.cpp(410): QueryThread::_runQuery Exception\n",
      "# EXCEPTION in query 221635: IndriRunQuery.cpp(410): QueryThread::_runQuery Exception\n",
      "# EXCEPTION in query 841212: IndriRunQuery.cpp(410): QueryThread::_runQuery Exception\n",
      "# EXCEPTION in query 943553: IndriRunQuery.cpp(410): QueryThread::_runQuery Exception\n",
      "# EXCEPTION in query 512541: IndriRunQuery.cpp(410): QueryThread::_runQuery Exception\n",
      "# EXCEPTION in query 841351: IndriRunQuery.cpp(410): QueryThread::_runQuery Exception\n",
      "# EXCEPTION in query 609540: IndriRunQuery.cpp(410): QueryThread::_runQuery Exception\n",
      "# EXCEPTION in query 603765: IndriRunQuery.cpp(410): QueryThread::_runQuery Exception\n",
      "# EXCEPTION in query 593650: IndriRunQuery.cpp(410): QueryThread::_runQuery Exception\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###skip\n",
    "import os\n",
    "\n",
    "Meta1Resultspath='/home/ubuntu/SpotifyChallenge/notebooks/Reranking/TestingQueryResults/Meta1/'\n",
    "Meta2Resultspath='/home/ubuntu/SpotifyChallenge/notebooks/Reranking/TestingQueryResults/Meta2/'\n",
    "QEPRFResultspath='/home/ubuntu/SpotifyChallenge/notebooks/Reranking/TestingQueryResults/QEPRF750/'\n",
    "\n",
    "Meta1Files=[Meta1Resultspath+x for x in os.listdir(Meta1Resultspath)]\n",
    "Meta2Files=[Meta2Resultspath+x for x in os.listdir(Meta2Resultspath)]\n",
    "QEPRFFiles=[QEPRFResultspath+x for x in os.listdir(QEPRFResultspath)]\n",
    "\n",
    "###skip\n",
    "import codecs\n",
    "def parseMetaFiles(path):\n",
    "    playlistId=path.split('/')[-1].split('.op')[0]\n",
    "    with codecs.open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.read().splitlines()\n",
    "    rank=0\n",
    "    resultSet=[]\n",
    "    for result in lines[1:]:\n",
    "        try:\n",
    "            rank=rank+1\n",
    "            splits=result.split('\\t')\n",
    "            score = splits[0]\n",
    "            trackid= splits[1]\n",
    "            resultSet.append((rank,trackid,score))\n",
    "        except:\n",
    "            print result\n",
    "            return \"QueryError\"\n",
    "    return(playlistId,resultSet)\n",
    "\n",
    "####skip\n",
    "Meta1Op=[]\n",
    "err1=[]\n",
    "Meta2Op=[]\n",
    "err2=[]\n",
    "for f in Meta1Files:\n",
    "    res=parseMetaFiles(f)\n",
    "    if res !=\"QueryError\":\n",
    "        Meta1Op.append(res)\n",
    "    else:\n",
    "        err1.append(f)\n",
    "for f in Meta2Files:\n",
    "    res=parseMetaFiles(f)\n",
    "    if res !=\"QueryError\":\n",
    "        Meta2Op.append(res)\n",
    "    else:\n",
    "        err2.append(f)\n",
    "        \n",
    "####skip\n",
    "import codecs\n",
    "def QEPRFParse(path):\n",
    "    playlistId=path.split('/')[-1].split('.op')[0]\n",
    "    with codecs.open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.read().splitlines()\n",
    "    inputQueries=lines[0].split('# query: ')[1].split()\n",
    "    resultSet=[]\n",
    "    pairResults= lines[1].split(' #weight(')[2].split(') )')[0].split('\"  ')\n",
    "    rank=0\n",
    "    for result in pairResults[:-1]:\n",
    "        try:\n",
    "            rank=rank+1\n",
    "            splits=result.split('\"')\n",
    "            score = splits[0].strip()\n",
    "            trackid= splits[1].strip()\n",
    "            resultSet.append((rank,trackid,score))\n",
    "        except:\n",
    "            print result\n",
    "            return \"QueryError\"\n",
    "    return(playlistId,inputQueries,resultSet)\n",
    "\n",
    "###skip\n",
    "QEPRFOp=[]\n",
    "err3=[]\n",
    "for f in QEPRFFiles:\n",
    "    res=QEPRFParse(f)\n",
    "    if res !=\"QueryError\":\n",
    "        QEPRFOp.append(res)\n",
    "    else:\n",
    "        err3.append(f)\n",
    "        \n",
    "###skip\n",
    "import pickle\n",
    "pidTrackMapping=pickle.load(open('./BiPartites/AllDataPidTrackListBipartite.pkl','rb'))\n",
    "\n",
    "####skip\n",
    "import pickle\n",
    "import os\n",
    "import codecs\n",
    "from random import shuffle\n",
    "\n",
    "pkl = os.listdir('./SplitsInformation/')\n",
    "\n",
    "count=0\n",
    "DS={}\n",
    "for fpkl in pkl:\n",
    "    if fpkl in ['testing25RandPid.pkl', 'testing25Pid.pkl', 'testing1Pid.pkl', 'testing100Pid.pkl', 'testing10Pid.pkl', 'testing5Pid.pkl', 'testing100RandPid.pkl']:\n",
    "        testType=fpkl.replace('.pkl','')\n",
    "        if 'Rand' in fpkl:\n",
    "            listLen=int(fpkl.split('testing')[1].split('Rand')[0])\n",
    "            qtype='Rand'\n",
    "        else :\n",
    "            listLen=int(fpkl.split('testing')[1].split('Pid')[0])\n",
    "            qtype='Normal'\n",
    "        testingPids=pickle.load(open('./SplitsInformation/'+fpkl,'rb'))\n",
    "        for pid in testingPids:\n",
    "            pid=str(pid)\n",
    "            referenceSet=[x.replace('spotify:track:','') for x in pidTrackMapping[pid]]\n",
    "            DS[pid]=(testType,qtype,listLen,referenceSet)\n",
    "\n",
    "####skip\n",
    "import pickle\n",
    "import os\n",
    "import codecs\n",
    "from random import shuffle\n",
    "\n",
    "pkl = os.listdir('./SplitsInformation/')\n",
    "testingTitleonlyPids=[]\n",
    "for fpkl in pkl:\n",
    "    if fpkl =='testingOnlyTitlePid.pkl':\n",
    "        testType=fpkl.replace('.pkl','')\n",
    "        listLen=0\n",
    "        qtype='Normal'\n",
    "        testingPids=pickle.load(open('./SplitsInformation/'+fpkl,'rb'))\n",
    "        for pid in testingPids:\n",
    "            pid=str(pid)\n",
    "            referenceSet=[x.replace('spotify:track:','') for x in pidTrackMapping[pid]]\n",
    "            DS[pid]=(testType,qtype,listLen,referenceSet)\n",
    "testingTitleonlyPids=[str(x) for x in testingPids]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TestFile='./Training/ExternalAndW2VAsFeatures-BigRecall-TestingFile750-2080.txt'\n",
    "with open(TestFile) as f:\n",
    "    test = f.readlines()\n",
    "\n",
    "PidTestTracks={}\n",
    "for l in test:\n",
    "    pid=l.split()[1].split(':')[1].strip()\n",
    "    track=l.split('#')[1].strip()\n",
    "    PidTestTracks.setdefault(pid,[]).append(track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4996"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(PidTestTracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Training/External2080Model-500Trees-NDCG20-tc-1-lr01-leaf50.txt Recall: 0.513076260943 NDCG: 0.30956540978 RPrec: 0.129767194973 Clicks: 4.4401521217\n"
     ]
    }
   ],
   "source": [
    "scoresfile='./Training/External2080Model-500Trees-NDCG20-tc-1-lr01-leaf50.txt'\n",
    "with open(scoresfile) as f:\n",
    "    scores = f.readlines()\n",
    "\n",
    "from collections import defaultdict\n",
    "from random import shuffle\n",
    "\n",
    "PidTracksScores={}\n",
    "for l in scores:\n",
    "    pid=l.split()[0].strip()\n",
    "    trackScore=l.split()[2].strip()\n",
    "    PidTracksScores.setdefault(pid,[]).append(float(trackScore))\n",
    "\n",
    "\n",
    "rerankedCandidates={}\n",
    "for pid,tracksList  in PidTestTracks.items():\n",
    "    scoresList=PidTracksScores[pid]\n",
    "    zippedPairs=zip(tracksList,scoresList)\n",
    "    shuffle(zippedPairs)\n",
    "    rerankedCandidates[pid]=[x[0] for x in sorted(zippedPairs, key=lambda x: x[1], reverse=True)]\n",
    "\n",
    "####continue here\n",
    "evalSets=[]\n",
    "for pl in QEPRFOp:\n",
    "    plId=pl[0]\n",
    "    if plId in rerankedCandidates:\n",
    "        exposed=pl[1]\n",
    "        candidates=rerankedCandidates[plId]\n",
    "        candidates=[x for x in candidates if x not in exposed]\n",
    "        refVals= DS[plId]\n",
    "        testtype=refVals[0]\n",
    "        orderType=refVals[1]\n",
    "        exposedLen=refVals[2]\n",
    "        playlist=refVals[3]\n",
    "        if orderType=='Normal':\n",
    "            groundTruth=playlist[exposedLen:]\n",
    "        else:\n",
    "            groundTruth=[x for x in playlist if x not in exposed]\n",
    "        evalSets.append((groundTruth, candidates[:500], testtype, exposedLen))\n",
    "\n",
    "for pl in Meta2Op:\n",
    "    plId=pl[0]\n",
    "    if plId in testingTitleonlyPids and plId in rerankedCandidates:\n",
    "        exposed=[]\n",
    "        candidates=rerankedCandidates[plId]\n",
    "        refVals= DS[plId]\n",
    "        testtype=refVals[0]\n",
    "        orderType=refVals[1]\n",
    "        exposedLen=refVals[2]\n",
    "        playlist=refVals[3]\n",
    "        groundTruth=playlist[exposedLen:]\n",
    "        evalSets.append((groundTruth, candidates[:500], testtype, exposedLen))\n",
    "####continue here\n",
    "\n",
    "'''\n",
    "r_precision(targets, predictions, k),\n",
    "                   ndcg(targets, predictions, k),\n",
    "                   playlist_extender_clicks(targets, predictions, k)\n",
    "'''\n",
    "\n",
    "indivSumsCounts= defaultdict(int)\n",
    "indivSumsRecall = defaultdict(int)\n",
    "indivSumsNdcg = defaultdict(int)\n",
    "indivSumsRprec = defaultdict(int)\n",
    "indivSumsClicks = defaultdict(int)\n",
    "globalNdcg=0\n",
    "globalRprec=0\n",
    "globalClicks=0\n",
    "globalRecall=0\n",
    "\n",
    "count=0\n",
    "for evalTuple in evalSets:\n",
    "    targets=evalTuple[0]\n",
    "    predictions=evalTuple[1]\n",
    "    testType=evalTuple[2]\n",
    "    tupNdcg=ndcg(targets,predictions,500)\n",
    "    tuprprec=r_precision(targets,predictions,500)\n",
    "    tupClicks=playlist_extender_clicks(targets,predictions,500)\n",
    "    globalNdcg+=tupNdcg\n",
    "    indivSumsNdcg[testType]+=tupNdcg\n",
    "    globalRprec+=tuprprec\n",
    "    indivSumsRprec[testType]+=tuprprec\n",
    "    globalClicks+=tupClicks\n",
    "    indivSumsClicks[testType]+=tupClicks\n",
    "\n",
    "    indivSumsCounts[testType]+=1\n",
    "\n",
    "    recallSetSize= len(set(predictions)&set(targets))\n",
    "    refSetSize=len(targets)\n",
    "    recall=recallSetSize*1.0/refSetSize\n",
    "    globalRecall+=recall\n",
    "    indivSumsRecall[testType]+=recall\n",
    "\n",
    "    count+=1\n",
    "for k, v in indivSumsCounts.items():\n",
    "    indivSumsRecall[k]=indivSumsRecall[k]/v\n",
    "    indivSumsNdcg[k]=indivSumsNdcg[k]/v\n",
    "    indivSumsRprec[k]=indivSumsRprec[k]/v\n",
    "    indivSumsClicks[k]=indivSumsClicks[k]/v\n",
    "\n",
    "print scoresfile , 'Recall:' , globalRecall/count,'NDCG:', globalNdcg/count, 'RPrec:', globalRprec/count,'Clicks:', globalClicks/count\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
