{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed10000\n",
      "processed20000\n",
      "processed30000\n",
      "processed40000\n",
      "processed50000\n",
      "processed60000\n",
      "processed70000\n",
      "processed80000\n",
      "processed90000\n",
      "processed100000\n",
      "processed110000\n",
      "processed120000\n",
      "processed130000\n",
      "processed140000\n",
      "processed150000\n",
      "processed160000\n",
      "processed170000\n",
      "processed180000\n",
      "processed190000\n",
      "processed200000\n",
      "processed210000\n",
      "processed220000\n",
      "processed230000\n",
      "processed240000\n",
      "processed250000\n",
      "processed260000\n",
      "processed270000\n",
      "processed280000\n",
      "processed290000\n",
      "processed300000\n",
      "processed310000\n",
      "processed320000\n",
      "processed330000\n",
      "processed340000\n",
      "processed350000\n",
      "processed360000\n",
      "processed370000\n",
      "processed380000\n",
      "processed390000\n",
      "processed400000\n",
      "processed410000\n",
      "processed420000\n",
      "processed430000\n",
      "processed440000\n",
      "processed450000\n",
      "processed460000\n",
      "processed470000\n",
      "processed480000\n",
      "processed490000\n",
      "processed500000\n",
      "processed510000\n",
      "processed520000\n",
      "processed530000\n",
      "processed540000\n",
      "processed550000\n",
      "processed560000\n",
      "processed570000\n",
      "processed580000\n",
      "processed590000\n",
      "processed600000\n",
      "processed610000\n",
      "processed620000\n",
      "processed630000\n",
      "processed640000\n",
      "processed650000\n",
      "processed660000\n",
      "processed670000\n",
      "processed680000\n",
      "processed690000\n",
      "processed700000\n",
      "processed710000\n",
      "processed720000\n",
      "processed730000\n",
      "processed740000\n",
      "processed750000\n",
      "processed760000\n",
      "processed770000\n",
      "processed780000\n",
      "processed790000\n",
      "processed800000\n",
      "processed810000\n",
      "processed820000\n",
      "processed830000\n",
      "processed840000\n",
      "processed850000\n",
      "processed860000\n",
      "processed870000\n",
      "processed880000\n",
      "processed890000\n",
      "processed900000\n",
      "processed910000\n",
      "processed920000\n",
      "processed930000\n",
      "processed940000\n",
      "processed950000\n",
      "processed960000\n",
      "processed970000\n",
      "processed980000\n",
      "processed990000\n",
      "processed1000000\n"
     ]
    }
   ],
   "source": [
    "#Generate BiPartites and save as Objects. \n",
    "import itertools as it\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import collections\n",
    "import os\n",
    "path='../../data'\n",
    "\n",
    "\n",
    "filenames = os.listdir(path)\n",
    "\n",
    "count=0\n",
    "\n",
    "AllDataPidTitleBipartite={}\n",
    "AllDataPidTrackListBipartite={}\n",
    "AllDataAlbumTrackSetBipartite={}\n",
    "AllDataArtistTrackSetBipartite={}\n",
    "AllDataTrackArtistBipartite={}\n",
    "AllDataTrackAlbumBipartite={}\n",
    "AllDataTrackNameBipartite={}\n",
    "AllDataAlbumNameBipartite={}\n",
    "AllDataAritstNameBipartite={}\n",
    "AllDataPidDescriptionBipartite={}\n",
    "\n",
    "for filename in sorted(filenames):\n",
    "    if filename.startswith(\"mpd.slice.\") and filename.endswith(\".json\"):\n",
    "        fullpath = os.sep.join((path, filename))\n",
    "        f = open(fullpath)\n",
    "        js = f.read()\n",
    "        f.close()\n",
    "        mpd_slice = json.loads(js)\n",
    "        for playlist in mpd_slice['playlists']:\n",
    "            playlistId=str(playlist['pid'])\n",
    "            playlistTracks=[]\n",
    "            playlistTitle=playlist['name']\n",
    "            for track in playlist['tracks']:\n",
    "                trackId=track['track_uri']\n",
    "                trackName=track['track_name']\n",
    "                trackArtistId=track['artist_uri']\n",
    "                trackArtistName=track['artist_name']\n",
    "                trackAlbumId=track['album_uri']\n",
    "                trackAlbumName=track['album_name']\n",
    "                playlistTracks.append(trackId)\n",
    "                AllDataAlbumTrackSetBipartite.setdefault(trackAlbumId,[]).append(trackId)\n",
    "                AllDataArtistTrackSetBipartite.setdefault(trackArtistId,[]).append(trackId)\n",
    "                AllDataTrackArtistBipartite[trackId]=trackArtistId\n",
    "                AllDataTrackAlbumBipartite[trackId]=trackAlbumId\n",
    "                AllDataTrackNameBipartite[trackId]=trackName\n",
    "                AllDataAlbumNameBipartite[trackAlbumId]=trackAlbumName\n",
    "                AllDataAritstNameBipartite[trackArtistId]=trackArtistName\n",
    "            AllDataPidTitleBipartite[playlistId]=playlistTitle\n",
    "            AllDataPidTrackListBipartite[playlistId]=playlistTracks\n",
    "            if 'description' in playlist:\n",
    "                AllDataPidDescriptionBipartite[playlistId]=playlist['description']\n",
    "            count=count+1\n",
    "            if count % 10000 ==0:\n",
    "                print 'processed' + str(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    ">>AllDataPidTitleBipartite={}\n",
    ">>AllDataPidTrackListBipartite={}\n",
    ">>AllDataAlbumTrackSetBipartite={}\n",
    "AllDataArtistTrackSetBipartite={}\n",
    "AllDataTrackArtistBipartite={}\n",
    "AllDataTrackAlbumBipartite={}\n",
    "AllDataTrackNameBipartite={}\n",
    "AllDataAlbumNameBipartite={}\n",
    "AllDataAritstNameBipartite={}\n",
    "AllDataPidDescriptionBipartite={}\n",
    "'''\n",
    "import pickle\n",
    "fileObj = open('./BiPartites/AllDataArtistTrackSetBipartite.pkl', 'w')\n",
    "pickle.dump(AllDataArtistTrackSetBipartite, fileObj)\n",
    "fileObj.close()\n",
    "import pickle\n",
    "fileObj = open('./BiPartites/AllDataTrackArtistBipartite.pkl', 'w')\n",
    "pickle.dump(AllDataTrackArtistBipartite, fileObj)\n",
    "fileObj.close()\n",
    "import pickle\n",
    "fileObj = open('./BiPartites/AllDataTrackAlbumBipartite.pkl', 'w')\n",
    "pickle.dump(AllDataTrackAlbumBipartite, fileObj)\n",
    "fileObj.close()\n",
    "import pickle\n",
    "fileObj = open('./BiPartites/AllDataTrackNameBipartite.pkl', 'w')\n",
    "pickle.dump(AllDataTrackNameBipartite, fileObj)\n",
    "fileObj.close()\n",
    "import pickle\n",
    "fileObj = open('./BiPartites/AllDataAlbumNameBipartite.pkl', 'w')\n",
    "pickle.dump(AllDataAlbumNameBipartite, fileObj)\n",
    "fileObj.close()\n",
    "import pickle\n",
    "fileObj = open('./BiPartites/AllDataAritstNameBipartite.pkl', 'w')\n",
    "pickle.dump(AllDataAritstNameBipartite, fileObj)\n",
    "fileObj.close()\n",
    "import pickle\n",
    "fileObj = open('./BiPartites/AllDataPidDescriptionBipartite.pkl', 'w')\n",
    "pickle.dump(AllDataPidDescriptionBipartite, fileObj)\n",
    "fileObj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 1000000, 734684, 295860, 2262292, 2262292, 2262292, 734684, 295860)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AllDataPidTitleBipartite), len(AllDataPidTrackListBipartite), len(AllDataAlbumTrackSetBipartite), len(AllDataArtistTrackSetBipartite), len(AllDataTrackArtistBipartite), len(AllDataTrackAlbumBipartite), len(AllDataTrackNameBipartite), len(AllDataAlbumNameBipartite), len(AllDataAritstNameBipartite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Indri Ready Indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Background Pids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#BuildIndriIndexs\n",
    "#Build750BackgroundIndexes/\n",
    "import pickle\n",
    "#backgroundPids=pickle.load(open('SplitsInformation/backgroundDataPids.pkl','rb'))\n",
    "backgroundPids=AllDataPidTitleBipartite.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(backgroundPids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "def entry(docId, docContent):\n",
    "    entry='<DOC>\\n<DOCNO>'+docId+'</DOCNO>\\n<TEXT>'+docContent+'\\n</TEXT>\\n</DOC>'\n",
    "    return entry\n",
    "\n",
    "def writeIndexToFile(indexDocsList,writePath):\n",
    "    count=0\n",
    "    errors=[]\n",
    "    with codecs.open(writePath,\"w\", encoding='utf-8') as f:\n",
    "        for line in indexDocsList:\n",
    "            try:\n",
    "                f.write(line +\"\\n\")\n",
    "                count=count+1\n",
    "            except:\n",
    "                errors.append(line)\n",
    "    print count\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Index Ready Doc - Pid as Document and Tracks of Playlist as Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#buildIndexPidAsDocTracksAsTerms\n",
    "\n",
    "backgroundpidDocs=[]\n",
    "for pid in backgroundPids:\n",
    "    trackList=AllDataPidTrackListBipartite[pid]\n",
    "    backgroundpidDocs.append(entry(str(pid), ' '.join([item.replace('spotify:track:','') for item in trackList])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writeIndexToFile(backgroundpidDocs,'./All1MDataIndexes/All1MIndexPidAsDocTracksAsTerms.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(backgroundpidDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed10000\n",
      "processed20000\n",
      "processed30000\n",
      "processed40000\n",
      "processed50000\n",
      "processed60000\n",
      "processed70000\n",
      "processed80000\n",
      "processed90000\n",
      "processed100000\n",
      "processed110000\n",
      "processed120000\n",
      "processed130000\n",
      "processed140000\n",
      "processed150000\n",
      "processed160000\n",
      "processed170000\n",
      "processed180000\n",
      "processed190000\n",
      "processed200000\n",
      "processed210000\n",
      "processed220000\n",
      "processed230000\n",
      "processed240000\n",
      "processed250000\n",
      "processed260000\n",
      "processed270000\n",
      "processed280000\n",
      "processed290000\n",
      "processed300000\n",
      "processed310000\n",
      "processed320000\n",
      "processed330000\n",
      "processed340000\n",
      "processed350000\n",
      "processed360000\n",
      "processed370000\n",
      "processed380000\n",
      "processed390000\n",
      "processed400000\n",
      "processed410000\n",
      "processed420000\n",
      "processed430000\n",
      "processed440000\n",
      "processed450000\n",
      "processed460000\n",
      "processed470000\n",
      "processed480000\n",
      "processed490000\n",
      "processed500000\n",
      "processed510000\n",
      "processed520000\n",
      "processed530000\n",
      "processed540000\n",
      "processed550000\n",
      "processed560000\n",
      "processed570000\n",
      "processed580000\n",
      "processed590000\n",
      "processed600000\n",
      "processed610000\n",
      "processed620000\n",
      "processed630000\n",
      "processed640000\n",
      "processed650000\n",
      "processed660000\n",
      "processed670000\n",
      "processed680000\n",
      "processed690000\n",
      "processed700000\n",
      "processed710000\n",
      "processed720000\n",
      "processed730000\n",
      "processed740000\n",
      "processed750000\n",
      "processed760000\n",
      "processed770000\n",
      "processed780000\n",
      "processed790000\n",
      "processed800000\n",
      "processed810000\n",
      "processed820000\n",
      "processed830000\n",
      "processed840000\n",
      "processed850000\n",
      "processed860000\n",
      "processed870000\n",
      "processed880000\n",
      "processed890000\n",
      "processed900000\n",
      "processed910000\n",
      "processed920000\n",
      "processed930000\n",
      "processed940000\n",
      "processed950000\n",
      "processed960000\n",
      "processed970000\n",
      "processed980000\n",
      "processed990000\n",
      "processed1000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import itertools as it\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import collections\n",
    "import os\n",
    "path='../../data'\n",
    "\n",
    "\n",
    "filenames = os.listdir(path)\n",
    "\n",
    "count=0\n",
    "\n",
    "pid750=[]\n",
    "AlbumTrackSetBipartite750={}\n",
    "ArtistTrackSetBipartite750={}\n",
    "\n",
    "for filename in sorted(filenames[:750]):\n",
    "    if filename.startswith(\"mpd.slice.\") and filename.endswith(\".json\"):\n",
    "        fullpath = os.sep.join((path, filename))\n",
    "        f = open(fullpath)\n",
    "        js = f.read()\n",
    "        f.close()\n",
    "        mpd_slice = json.loads(js)\n",
    "        for playlist in mpd_slice['playlists']:\n",
    "            playlistId=str(playlist['pid'])\n",
    "            pid750.append(playlistId)\n",
    "            \n",
    "            for track in playlist['tracks']:\n",
    "                trackId=track['track_uri']\n",
    "                trackName=track['track_name']\n",
    "                trackArtistId=track['artist_uri']\n",
    "                trackArtistName=track['artist_name']\n",
    "                trackAlbumId=track['album_uri']\n",
    "                trackAlbumName=track['album_name']\n",
    "                \n",
    "                AlbumTrackSetBipartite750.setdefault(trackAlbumId,[]).append(trackId)\n",
    "                ArtistTrackSetBipartite750.setdefault(trackArtistId,[]).append(trackId)\n",
    "            count=count+1\n",
    "            if count % 10000 ==0:\n",
    "                print 'processed' + str(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pid750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "albumDocsNorm=[]\n",
    "albumDocsNonNorm=[]\n",
    "for albumid, tracks in AlbumTrackSetBipartite750.items():\n",
    "    normTracks=list(set(tracks))\n",
    "    albumDocsNonNorm.append(entry(albumid.strip(), ' '.join(list([item.replace('spotify:track:','') for item in tracks]))))\n",
    "    albumDocsNorm.append(entry(albumid.strip(), ' '.join(list([item.replace('spotify:track:','') for item in normTracks]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "734684\n",
      "734684\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writeIndexToFile(albumDocsNorm,'./All1MDataIndexes/1MIndexAlbumAsDocNormTracksSetAsTerms.txt')\n",
    "writeIndexToFile(albumDocsNonNorm,'./All1MDataIndexes/1MIndexAlbumAsDocNonNormTracksListAsTerms.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "artistDocsNorm=[]\n",
    "artistDocsNonNorm=[]\n",
    "for albumid, tracks in ArtistTrackSetBipartite750.items():\n",
    "    normTracks=list(set(tracks))\n",
    "    artistDocsNonNorm.append(entry(albumid.strip(), ' '.join(list([item.replace('spotify:track:','') for item in tracks]))))\n",
    "    artistDocsNorm.append(entry(albumid.strip(), ' '.join(list([item.replace('spotify:track:','') for item in normTracks]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295860"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ArtistTrackSetBipartite750.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295860\n",
      "295860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writeIndexToFile(artistDocsNorm,'./All1MDataIndexes/1MIndexArtistsAsDocNormTracksSetAsTerms.txt')\n",
    "writeIndexToFile(artistDocsNonNorm,'./All1MDataIndexes/1MIndexArtistsAsDocNonNormTracksListAsTerms.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "stop='../stopwords.txt'\n",
    "emojiSynonyms='../emojiWords.txt'\n",
    "stopList=[]\n",
    "emojiMap={}\n",
    "emojis=[]\n",
    "f=codecs.open(stop,'r', encoding='utf-8')\n",
    "\n",
    "for line in f.readlines():\n",
    "    stopList.append(line.strip())\n",
    "\n",
    "f=codecs.open(emojiSynonyms,'r', encoding='utf-8')\n",
    "for line in f.readlines():\n",
    "    emoji, meaning= line.strip().split()\n",
    "    emojiMap[emoji.strip()]=' '.join(meaning.strip().split(','))\n",
    "    emojis.append(emoji.strip())\n",
    "        \n",
    "def NormalizeDates(title):    \n",
    "    fullDecades=['1930','1940','1950','1960','1970','1980','1990','2000','2010']\n",
    "    truncDecades=['30','40','50','60','70','80','90','00','10']\n",
    "    year=['2001','2002','2003','2004','2005','2006','2007','2008','2009','2011','2012','2013','2014','2015','2016','2017']\n",
    "    truncYear=['02','03','04','05','06','07','08','09','11','12','13','14','15','16','17']\n",
    "    seasons=['spring', 'summer','fall','winter','sommer', 'autumn', 'verano' ]\n",
    "    months=['january','jan','february','feb','march','april','may','june','july','august','aug','spetember','sep','october','oct','novemeber','nov','december','dec']\n",
    "    monthPairs=[('january','jan'),('february','feb'),('august','aug'),('september','sept'), ('september','sep'),('october','oct'),('november','nov'),('december','dec')]\n",
    "\n",
    "    title=title.lower().strip()\n",
    "    \n",
    "    for decade in fullDecades:\n",
    "        if decade in title:\n",
    "            newTitle=''\n",
    "            for word in title.split():\n",
    "                if decade in word:\n",
    "                    word=word.replace(decade+\"'\",decade)\n",
    "                    word=word.replace(decade+\"s\",decade)\n",
    "                    word=word.replace(decade,' '+decade+'s ')\n",
    "                newTitle=newTitle+' '+word\n",
    "            #print title+'------'+newTitle\n",
    "            title=newTitle.strip()\n",
    "\n",
    "    for truncDecade in truncDecades:\n",
    "        if truncDecade in title and ('19'+truncDecade not in title) and ('20'+truncDecade not in title):\n",
    "            newTitle=''\n",
    "            for word in title.split():\n",
    "                if truncDecade+\"'\" in word  or truncDecade+\"s\" in word or truncDecade+u\"´s\" in word or truncDecade+\"'s\" in word or truncDecade+\" s \" in word or truncDecade+\"ies\" in word:\n",
    "                    word=word.replace(truncDecade+\"'\",truncDecade)\n",
    "                    word=word.replace(truncDecade+\"s\",truncDecade)\n",
    "                    word=word.replace(truncDecade+\" s\",truncDecade)\n",
    "                    word=word.replace(truncDecade+u\"´s\",truncDecade)\n",
    "                    word=word.replace(truncDecade+\"ies\",truncDecade)\n",
    "                    if truncDecade in truncDecades[:7]:\n",
    "                        century='19'\n",
    "                    else:\n",
    "                        century='20'\n",
    "                    word=word.replace(truncDecade,' '+century+truncDecade+'s ')\n",
    "                newTitle=newTitle+' '+word\n",
    "            title=newTitle.strip()\n",
    "    if title.strip() in ['70-80','70,80,90','90/00', '70 80 90', '90 00', '80 90', '80-90','90-00', '60-70', '70 80']:\n",
    "        for truncDecade in truncDecades:\n",
    "            if truncDecade in truncDecades[:7]:\n",
    "                century='19'\n",
    "            else:\n",
    "                century='20'\n",
    "            title=title.replace(truncDecade,' '+century+truncDecade+'s ' )\n",
    "    title=title.strip()\n",
    "    for yr in year:\n",
    "        if yr in title:\n",
    "            title=title.replace(yr,' '+yr+' ')\n",
    "            \n",
    "    for yr in truncYear:\n",
    "        if yr in title and '20'+yr not in title:\n",
    "            if \"'\"+yr in title:\n",
    "                #print title\n",
    "                title=title.replace(\"'\"+yr,' 20'+yr+' ')\n",
    "                #print title\n",
    "            if yr+\"'\" in title:\n",
    "                #print title\n",
    "                title=title.replace(yr+\"'\",' 20'+yr+' ')\n",
    "                #print title\n",
    "            if '2k'+yr in title or '2K'+yr in title:\n",
    "                #print title\n",
    "                title=title.replace('2k'+yr,' 20'+yr+' ')\n",
    "                #print title\n",
    "            if '-'+yr in title :\n",
    "                #print title\n",
    "                title=title.replace('-'+yr,' 20'+yr+' ')\n",
    "                #print title\n",
    "        if yr in title and '20'+yr not in title:\n",
    "            for season in seasons:\n",
    "                if season in title:\n",
    "                    #print title\n",
    "                    title=title.replace(yr,' 20'+yr+' ')\n",
    "                    #print title\n",
    "            for month in months:\n",
    "                if month in title and '20'+yr not in title:\n",
    "                    #print title\n",
    "                    title=title.replace(yr,' 20'+yr+' ')\n",
    "                    #print '-'+title\n",
    "    for month,shortMonth in monthPairs:\n",
    "        newTitle=''\n",
    "        if shortMonth in title and month not in title:\n",
    "            #print title\n",
    "            for word in title.split():\n",
    "                if shortMonth== word:\n",
    "                    newTitle=newTitle+' '+month\n",
    "                else:\n",
    "                    newTitle=newTitle+' '+word\n",
    "            title=newTitle\n",
    "            #print '-'+title\n",
    "    title=title.strip()\n",
    "    title=' '.join(title.split())\n",
    "    return title\n",
    "\n",
    "def handleEmojis(title):\n",
    "    for emo in emojis:\n",
    "        if emo in title:\n",
    "            title=title.replace(emo,' ')\n",
    "            title=title+' '+emojiMap[emo]+' '\n",
    "            \n",
    "    title=title.replace(u'\\U0001f3fc','')\n",
    "    title=title.replace(u'\\U0001f3fd','')\n",
    "    title=title.replace(u'\\U0001f3fb','')\n",
    "    title=title.replace(u'\\U0001f3fe','')\n",
    "    title=title.replace(u'\\u200d','')\n",
    "    title=title.replace(u'\\ufe0f','')\n",
    "    title=title.replace(u'oshrug','shrug')\n",
    "    title=title.replace(u'\\ufffd','')\n",
    "    title=title.replace(u'\\U0001f37b','')\n",
    "    title=title.replace(u'\\u200d','')\n",
    "    title=title.replace(u'\\u2640','')\n",
    "    title=title.replace(u'\\u2642','')\n",
    "    title=title.replace(u'\\U0001f3b6','')\n",
    "    title=title.replace(u'\\u2728','')\n",
    "    title=title.replace(u'\\U0001f449','')\n",
    "    title=title.replace(u'\\U0001f3ff','')\n",
    "    title=title.replace(u'\\U0001f38a','')\n",
    "    title=title.replace(u'\\U0001f445','')\n",
    "    title=title.replace(u'\\U0001f608','')\n",
    "    title=title.replace(u'\\U0001f381','')\n",
    "    title=title.replace(u'\\U0001f60f','')\n",
    "    title=title.replace(u'\\U0001f4a8','')\n",
    "    title=title.replace(u'�','')\n",
    "    title=title.replace('<3',' heart love ')\n",
    "    title=title.replace(':)',' smile happy ')\n",
    "    title=title.replace(';)',' smirk happy ')\n",
    "    title=title.replace(':-)',' smile happy ')\n",
    "    title=title.replace(': )',' smile happy ')\n",
    "    title=title.replace(u'😋',' smile happy ')\n",
    "    title=title.replace(u'\\u263a\\ufe0f',' smile happy ')\n",
    "    title=title.replace('r&b',' randb ')\n",
    "    title=title.replace('r & b',' randb ')\n",
    "    title=title.replace('rnb',' randb ')\n",
    "    title=title.replace(u'•',' ')\n",
    "    title=title.replace(u'\\u263a\\ufe0f',' death poison ')\n",
    "    title=title.replace(u'\\u2615\\ufe0f',' coffee tea morning ')\n",
    "    title=title.replace(u'💩',' poop ')\n",
    "    title=title.strip()\n",
    "    title=' '.join(title.split())\n",
    "    return title\n",
    "# # ' & - $ . : ! / () * ,\n",
    "import re\n",
    "def normalize_nameTitle(name):\n",
    "    name = name.lower()\n",
    "    name = NormalizeDates(name)\n",
    "    name = handleEmojis(name)\n",
    "    name = name.replace('happysmile','happy smile')\n",
    "    name = re.sub(r\"[.,\\/#\\'?\\&\\-!$%\\^\\*;:{}=\\_`~()@]\", ' ', name)\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "    return name\n",
    "def normalize_name(name):\n",
    "    name = name.lower()\n",
    "    name = re.sub(r\"[.,\\/#\\'?\\&\\-!$%\\^\\*;:{}=\\_`~()@]\", ' ', name)\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'normalize 2016 me smile happy heart love'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_nameTitle('normalize2016 me? :) <3 .....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed10000\n",
      "processed20000\n",
      "processed30000\n",
      "processed40000\n",
      "processed50000\n",
      "processed60000\n",
      "processed70000\n",
      "processed80000\n",
      "processed90000\n",
      "processed100000\n",
      "processed110000\n",
      "processed120000\n",
      "processed130000\n",
      "processed140000\n",
      "processed150000\n",
      "processed160000\n",
      "processed170000\n",
      "processed180000\n",
      "processed190000\n",
      "processed200000\n",
      "processed210000\n",
      "processed220000\n",
      "processed230000\n",
      "processed240000\n",
      "processed250000\n",
      "processed260000\n",
      "processed270000\n",
      "processed280000\n",
      "processed290000\n",
      "processed300000\n",
      "processed310000\n",
      "processed320000\n",
      "processed330000\n",
      "processed340000\n",
      "processed350000\n",
      "processed360000\n",
      "processed370000\n",
      "processed380000\n",
      "processed390000\n",
      "processed400000\n",
      "processed410000\n",
      "processed420000\n",
      "processed430000\n",
      "processed440000\n",
      "processed450000\n",
      "processed460000\n",
      "processed470000\n",
      "processed480000\n",
      "processed490000\n",
      "processed500000\n",
      "processed510000\n",
      "processed520000\n",
      "processed530000\n",
      "processed540000\n",
      "processed550000\n",
      "processed560000\n",
      "processed570000\n",
      "processed580000\n",
      "processed590000\n",
      "processed600000\n",
      "processed610000\n",
      "processed620000\n",
      "processed630000\n",
      "processed640000\n",
      "processed650000\n",
      "processed660000\n",
      "processed670000\n",
      "processed680000\n",
      "processed690000\n",
      "processed700000\n",
      "processed710000\n",
      "processed720000\n",
      "processed730000\n",
      "processed740000\n",
      "processed750000\n",
      "processed760000\n",
      "processed770000\n",
      "processed780000\n",
      "processed790000\n",
      "processed800000\n",
      "processed810000\n",
      "processed820000\n",
      "processed830000\n",
      "processed840000\n",
      "processed850000\n",
      "processed860000\n",
      "processed870000\n",
      "processed880000\n",
      "processed890000\n",
      "processed900000\n",
      "processed910000\n",
      "processed920000\n",
      "processed930000\n",
      "processed940000\n",
      "processed950000\n",
      "processed960000\n",
      "processed970000\n",
      "processed980000\n",
      "processed990000\n",
      "processed1000000\n"
     ]
    }
   ],
   "source": [
    "import itertools as it\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import collections\n",
    "import os\n",
    "path='../../data'\n",
    "\n",
    "\n",
    "filenames = os.listdir(path)\n",
    "\n",
    "count=0\n",
    "\n",
    "pid750=[]\n",
    "\n",
    "TrackIdTitle750={}\n",
    "TitleTrackId750={}\n",
    "\n",
    "TrackIdArtistName750={}\n",
    "TrackIdAbumName750={}\n",
    "TrackIdTrackName750={}\n",
    "\n",
    "AlbumTrackSetBipartite750={}\n",
    "ArtistTrackSetBipartite750={}\n",
    "\n",
    "for filename in filenames:\n",
    "    if filename.startswith(\"mpd.slice.\") and filename.endswith(\".json\"):\n",
    "        fullpath = os.sep.join((path, filename))\n",
    "        f = open(fullpath)\n",
    "        js = f.read()\n",
    "        f.close()\n",
    "        mpd_slice = json.loads(js)\n",
    "        for playlist in mpd_slice['playlists']:\n",
    "            playlistId=str(playlist['pid'])\n",
    "            pid750.append(playlistId)\n",
    "            pname=playlist['name']\n",
    "            normpName=normalize_nameTitle(pname).strip()\n",
    "            if normpName=='':\n",
    "                normpName='emptyTitle'\n",
    "            for track in playlist['tracks']:\n",
    "                trackId=track['track_uri']\n",
    "                trackName=track['track_name']\n",
    "                trackArtistId=track['artist_uri']\n",
    "                trackArtistName=track['artist_name']\n",
    "                trackAlbumId=track['album_uri']\n",
    "                trackAlbumName=track['album_name']\n",
    "                \n",
    "                TrackIdTitle750.setdefault(trackId,[]).append(normpName)#--Done\n",
    "                TitleTrackId750.setdefault(normpName,[]).append(trackId)#--Done\n",
    "                \n",
    "                TrackIdArtistName750[trackId]=trackArtistName#--meta2\n",
    "                TrackIdAbumName750[trackId]=trackAlbumName#--meta2\n",
    "                TrackIdTrackName750[trackId]=trackName#--meta2\n",
    "                \n",
    "                                \n",
    "                AlbumTrackSetBipartite750.setdefault(trackAlbumId,[]).append(trackId)#done\n",
    "                ArtistTrackSetBipartite750.setdefault(trackArtistId,[]).append(trackId)#done\n",
    "                \n",
    "            count=count+1\n",
    "            if count % 10000 ==0:\n",
    "                print 'processed' + str(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2262292"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TrackIdTitle750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titleDocsNorm=[]\n",
    "titletDocsNonNorm=[]\n",
    "for title, tracks in TitleTrackId750.items():\n",
    "    normTracks=list(set(tracks))\n",
    "    titletDocsNonNorm.append(entry(title.strip(), ' '.join(list([item.replace('spotify:track:','') for item in tracks]))))\n",
    "    titleDocsNorm.append(entry(title.strip(), ' '.join(list([item.replace('spotify:track:','') for item in normTracks]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18303\n",
      "18303\n"
     ]
    }
   ],
   "source": [
    "err1=writeIndexToFile(titleDocsNorm,'./All1MDataIndexes/1MIndexTitlesAsDocNormTracksSetAsTerms.txt')\n",
    "err2=writeIndexToFile(titletDocsNonNorm,'./All1MDataIndexes/1MIndexTitlesAsDocNonNormTracksListAsTerms.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "def titlePlusBigrams(title):\n",
    "    bigrm = list(nltk.bigrams(title.split()))\n",
    "    bis=''\n",
    "    for x1,x2 in bigrm:\n",
    "        bis=bis+' '+x1+x2\n",
    "    return title+' '+bis.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18303"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(titleDocsNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trackTitleDocs=[]\n",
    "for trackId, titleList in  TrackIdTitle750.items():\n",
    "    truncTrackId=trackId.replace('spotify:track:','')\n",
    "    concatTitle=''\n",
    "    for title in titleList:\n",
    "        concatTitle=concatTitle+' '+titlePlusBigrams(title)\n",
    "    trackTitleDocs.append(entry(truncTrackId.strip(), concatTitle))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2262292\n"
     ]
    }
   ],
   "source": [
    "err3=writeIndexToFile(trackTitleDocs,'./All1MDataIndexes/1MIndexTracksAsDocTitlesAsTerms.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TrackIdArtistName750[trackId]=trackArtistName#--meta2\n",
    "#TrackIdAbumName750[trackId]=trackAlbumName#--meta2\n",
    "#TrackIdTrackName750[trackId]=trackName#--meta2\n",
    "\n",
    "meta2trackDocs=[]\n",
    "for trackId, trackname in TrackIdTrackName750.items():\n",
    "    truncTrackId=trackId.replace('spotify:track:','')\n",
    "    normTrackName= normalize_nameTitle(trackname)\n",
    "    normAlbumName= normalize_nameTitle(TrackIdAbumName750[trackId])\n",
    "    normArtistName= normalize_nameTitle(TrackIdArtistName750[trackId])\n",
    "    meta2trackDocs.append(entry(truncTrackId,normTrackName+' '+normAlbumName+' '+normArtistName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2262292\n"
     ]
    }
   ],
   "source": [
    "err4=writeIndexToFile(meta2trackDocs,'./All1MDataIndexes/1MIndexTracksAsDocMeta2AsTerms.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2262292"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(meta2trackDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'<DOC>\\n<DOCNO>2d1caLRgavmHI07Hv6V3kM</DOCNO>\\n<TEXT>el bazucaso en vivo desde hermosillo sonora 2007 20 corridos poderosos el tigrillo palma\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>76ZRIwL6xXyPKtOIkUMAND</DOCNO>\\n<TEXT>mine would be you modern day lullabies baby sleeptime\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>0CbF79Ojec7oAzi0SJs7Cn</DOCNO>\\n<TEXT>faded love countrypolitan piano the first four albums floyd cramer\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>3VrBBMJMx9TZBUwoIf5MOO</DOCNO>\\n<TEXT>high lifted everybody lift em a worshipper s perspective earnest pugh\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>59LSmk4lneiptZrj8TZujq</DOCNO>\\n<TEXT>nostalgia peste hierophant\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>5h1ftnPNSUAFz0R8D8f2w4</DOCNO>\\n<TEXT>dulce amor santa domingo vol 2 bachata band\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>1y0dKXj8I27cYh1I15M5Bd</DOCNO>\\n<TEXT>birdhouse booty feet booty feet\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>5ckacg1TPKZKkUwfFmSote</DOCNO>\\n<TEXT>about this thing about this thing young franco\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>0pJ58HJso6HgcOEFx8YdX4</DOCNO>\\n<TEXT>barbra streisand barbra streisand duck sauce\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>7qmkQm8eb5heO2p3ErRjH5</DOCNO>\\n<TEXT>mr kopp d u c booba\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>1UyDoHOOumfMYSG0v8DWUx</DOCNO>\\n<TEXT>if down was up stingray kenny brown\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>5kQFh2BQKkAxJ7QxuvSomQ</DOCNO>\\n<TEXT>roll away your stone noah guthrie the covers vol 4 noah guthrie\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>2dMEubyfCxT8c7W9UJHX44</DOCNO>\\n<TEXT>morceau 20 music from lebanon and the levant of the arab renaissance the legacy of m\\u012bh\\u0101 \\u012bl ma\\u0161\\u0161\\u0101qa 1800 1888 nidaa abou mrad\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>0TsyBAbLIcJXLG9CsHwZ2e</DOCNO>\\n<TEXT>caracolito mania 2050 grupo mania\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>4gzQ4MFhWw5GtVBypzz4L0</DOCNO>\\n<TEXT>open your mind feat jyager 420 mixtape black the ripper\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>4kZnIlH1CvH8FYNSSxodpl</DOCNO>\\n<TEXT>very early california here i come bill evans\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>50nYJH35btGIj3BCQA66r2</DOCNO>\\n<TEXT>the duel new ammo karl denson s tiny universe\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>34dyCSrYuoy997wOgUDi7R</DOCNO>\\n<TEXT>dominoes back to me howie d\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>5fUZNS9QZXOg0aYjnIjr1H</DOCNO>\\n<TEXT>calling in love feat beenzino calling in love suran\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>66TccBhXxJEdQDtyycvlqV</DOCNO>\\n<TEXT>rebirth this is epic music vol 1 hi finesse\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>5iGZw2g714T32f3eYN4vq1</DOCNO>\\n<TEXT>i am the rose of sharon early american choral music vol 1 anthems and fuging tunes by william billings william billings\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>1S0VZwSJFsJ8ArIhGJoh5H</DOCNO>\\n<TEXT>glasgow glasgow the snuts\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>3Jl4h2fmdKJiJtl7oi1M69</DOCNO>\\n<TEXT>all of woman album mix all of woman robin auld\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>1W82BNVnSwL8YTNOh5hI2U</DOCNO>\\n<TEXT>fuchur original mix chimera ep sons of maria\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>534CNXL9vJLdd7gBwtMZQB</DOCNO>\\n<TEXT>angelito de mi guarda nuestros inicios puro durango conjunto atardecer\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>3TniqQQdO0nHKpSA1lDIja</DOCNO>\\n<TEXT>the inside it s not the end throwing gravity\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>4Ul09N1XCjECQWxVqqC1kp</DOCNO>\\n<TEXT>528hz the miracle tone 528hz the miracle tone meditative mind\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>5qEewVo7AVtlZaYZnIcL4H</DOCNO>\\n<TEXT>smile feat marcus bridge devil may care annisokay\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>6QK75nMmWZVV6Bi1QfYc0e</DOCNO>\\n<TEXT>i ain t heard of that remix instrumental i ain t heard of that remix slim thug\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>44skZlcazInKjDvHFQbMZ3</DOCNO>\\n<TEXT>like me feat raw resse jay love like me feat raw resse jay love trenton p\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>22r1D023UHuDMRd7SoYdTy</DOCNO>\\n<TEXT>y el amor se\\xf1ora ellas cantan a serrat eugenia leon\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>5CdIoIhqBMiQyALeOPIJdF</DOCNO>\\n<TEXT>me v angry mob me v angry mob world inferno friendship society\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>3u3t5n11ZAK8SqDVxP76Fo</DOCNO>\\n<TEXT>midway the early years acid king\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>3Ke53ZnZDN9FoXMJjFumGw</DOCNO>\\n<TEXT>yesterday piano bar piano bar\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>0axfEGtYmX6ti8r50vDGU4</DOCNO>\\n<TEXT>my girl has gone second version the complete motown singles volume 5 1965 smokey robinson the miracles\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>7cssjZec7F1MwbiNPNEGbQ</DOCNO>\\n<TEXT>erase \\ub108\\ub97c \\uc9c0\\uc6cc\\ubcf8\\ub2e4 erase \\ub108\\ub97c \\uc9c0\\uc6cc\\ubcf8\\ub2e4 big star \\ube45\\uc2a4\\ud0c0\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>0jUWkZEaj8y1jsVtPB7T9t</DOCNO>\\n<TEXT>i saw jesus in you i saw jesus in you ron hamilton\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>1mmR2JZqWeVjdRu6nlgXrI</DOCNO>\\n<TEXT>daylight is short in fall the art of how to fall rebekka bakken\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>2JgTpUZsbG3JMWbex1dxpA</DOCNO>\\n<TEXT>a foggy day rudy van gelder edition here tis rvg edition lou donaldson\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>5kRxTvTJAU28hvuDf0fxHS</DOCNO>\\n<TEXT>disaster boy night shades cobra starship\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>535j0bn26A24ki8Nfx3zmH</DOCNO>\\n<TEXT>sonata for clarinet and piano no 2 in e flat major op 120 sonata for clarinet and piano no 2 in e flat major op 120 i allegro amabile brahms sonatas for clarinet and piano op 120 johannes brahms\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>78phqm4PwabWE2eDBY6Fnu</DOCNO>\\n<TEXT>you ll never again be mine the lost notebooks of hank williams levon helm\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>75MYQEw0qKFFGzoa0XUDae</DOCNO>\\n<TEXT>late night more sand than money oogee wawa\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>4qMdii1kvxoxgQcToDchhN</DOCNO>\\n<TEXT>html sour swim team kush yung aol\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>75IBbgM5H6vdsvScv9CZkg</DOCNO>\\n<TEXT>redcon 1 team feat sabo arez panda j deuce bq hk ub stephen hobbs kuzzn bank adam jr therapy session soldier hard\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>2rApzetgClDYTDKx5TvBMY</DOCNO>\\n<TEXT>one word one word single by the rivers\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>2i1C8QkVZWpk4szDwKJNtM</DOCNO>\\n<TEXT>the goddess did not produce a shadow the stars outnumber the dead the heartland\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>6MUTvByaLUJ5ReuQ69UvfP</DOCNO>\\n<TEXT>kjurrt and they have escaped the weight of darkness \\xf3lafur arnalds\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>6uHINBRFcEJIB7YyArsTCf</DOCNO>\\n<TEXT>battleaxe dope sick madchild\\n</TEXT>\\n</DOC>',\n",
       " u'<DOC>\\n<DOCNO>0wK9TxKkzLzqnDi1NLctnY</DOCNO>\\n<TEXT>iris iris friendzone\\n</TEXT>\\n</DOC>']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta2trackDocs[:50]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
